{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# General E(2)-Equivariant Steerable CNNs  -  A concrete example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import RandomRotation, Pad, Resize, ToTensor, Compose\n",
    "from torchsummary import summary\n",
    "\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build a **Steerable CNN** and try it MNIST.\n",
    "\n",
    "Let's also use a group a bit larger: we now build a model equivariant to $8$ rotations.\n",
    "We indicate the group of $N$ discrete rotations as $C_N$, i.e. the **cyclic group** of order $N$.\n",
    "In this case, we will use $C_8$.\n",
    "\n",
    "Because the inputs are still gray-scale images, the input type of the model is again a *scalar field*.\n",
    "\n",
    "However, internally we use *regular fields*: this is equivalent to a *group-equivariant convolutional neural network*.\n",
    "\n",
    "Finally, we build *invariant* features for the final classification task by pooling over the group using *Group Pooling*.\n",
    "\n",
    "The final classification is performed by a two fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "Here is the definition of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C8SteerableCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes=10, cyclic_group=8):\n",
    "        self.cyclic_group=cyclic_group\n",
    "        \n",
    "        super(C8SteerableCNN, self).__init__()\n",
    "        \n",
    "        # the model is equivariant under rotations by 45 degrees, modelled by C8\n",
    "        self.r2_act = gspaces.Rot2dOnR2(N=self.cyclic_group)\n",
    "        #print(\"self.r2_act\", self.r2_act)\n",
    "        \n",
    "        # the input image is a scalar field, corresponding to the trivial representation\n",
    "        in_type = nn.FieldType(self.r2_act, 3*[self.r2_act.trivial_repr])\n",
    "        #print(\"in_type\", in_type)\n",
    "        \n",
    "        # we store the input type for wrapping the images into a geometric tensor during the forward pass\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        # convolution 1\n",
    "        # first specify the output type of the convolutional layer\n",
    "        # we choose 24 feature fields, each transforming under the regular representation of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 24*[self.r2_act.regular_repr])\n",
    "        #print(\"out_type\", out_type)\n",
    "        self.block1 = nn.SequentialModule(\n",
    "            nn.MaskModule(in_type, 80, margin=1),\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=7, padding=1, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        print('block1', self.block1.out_type.size)\n",
    "        \n",
    "        # convolution 2\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block1.out_type\n",
    "        # the output type of the second convolution layer are 48 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block2 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        print('block2', self.block2.out_type.size)\n",
    "        self.pool1 = nn.SequentialModule(\n",
    "            nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        )\n",
    "        print('pool1', self.pool1.out_type.size)\n",
    "        \n",
    "        # convolution 3\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block2.out_type\n",
    "        # the output type of the third convolution layer are 48 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block3 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        print('block3', self.block3.out_type.size)\n",
    "        \n",
    "        # convolution 4\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block3.out_type\n",
    "        # the output type of the fourth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block4 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        print('block4', self.block4.out_type.size)\n",
    "        self.pool2 = nn.SequentialModule(\n",
    "            nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        )\n",
    "        print('pool2', self.pool2.out_type.size)\n",
    "        \n",
    "        # convolution 5\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block4.out_type\n",
    "        # the output type of the fifth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block5 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        print('block5', self.block5.out_type.size)\n",
    "        \n",
    "        # convolution 6\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block5.out_type\n",
    "        # the output type of the sixth convolution layer are 64 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 64*[self.r2_act.regular_repr])\n",
    "        self.block6 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        print('block6', self.block6.out_type.size)\n",
    "        self.pool3 = nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=1, padding=0)\n",
    "        print('pool3', self.pool3.out_type.size)\n",
    "        \n",
    "        self.gpool = nn.GroupPooling(out_type) # pool3.out_type\n",
    "        \n",
    "        # number of output channels\n",
    "        c = self.gpool.out_type.size\n",
    "        print('gpool', c)\n",
    "        \n",
    "        # Fully Connected\n",
    "        self.fully_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(c*13*13, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.ELU(inplace=True),\n",
    "            torch.nn.Linear(64, n_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # wrap the input tensor in a GeometricTensor\n",
    "        # (associate it with the input type)\n",
    "        #print(\"input.shape\", input.shape)\n",
    "        x = nn.GeometricTensor(input, self.input_type)\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        \n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        \n",
    "        # pool over the spatial dimensions\n",
    "        x = self.pool3(x)\n",
    "        #print(\"x.shape1\", x.shape)\n",
    "        \n",
    "        # pool over the group\n",
    "        x = self.gpool(x)\n",
    "        #print(\"x.shape2\", x.shape)\n",
    "\n",
    "        # unwrap the output GeometricTensor\n",
    "        # (take the Pytorch tensor and discard the associated representation)\n",
    "        x = x.tensor\n",
    "        #print(\"x.shape3\", x.shape)\n",
    "        \n",
    "        # classify with the final fully connected layers)\n",
    "        x = self.fully_net(x.reshape(x.shape[0], -1))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not rotationally-equivariant architecture, mimics above as closely as possible\n",
    "class NonRECNN(torch.nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # convolution 1\n",
    "        self.block1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=24, kernel_size=7, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(num_features=24),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 2\n",
    "        self.block2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=24, out_channels=48, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            torch.nn.BatchNorm2d(num_features=48),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool1 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # convolution 3\n",
    "        self.block3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=48, out_channels=48, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            torch.nn.BatchNorm2d(num_features=48),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 4\n",
    "        self.block4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=48, out_channels=96, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            torch.nn.BatchNorm2d(num_features=96),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # convolution 5\n",
    "        self.block5 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=96, out_channels=96, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            torch.nn.BatchNorm2d(num_features=96),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 6\n",
    "        self.block6 = torch.nn.Sequential(\n",
    "            # NOTE 1: changed padding=1 to padding=0 in this layer to help match input size of fc layer\n",
    "            torch.nn.Conv2d(in_channels=96, out_channels=64, kernel_size=5, stride=1, padding=0, bias=False),\n",
    "            torch.nn.BatchNorm2d(num_features=64),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool3 = torch.nn.AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
    "        # NOTE 2: added another avgpool2d to match input size of fc layer\n",
    "        self.pool4 = torch.nn.AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
    "#        self.pool5 = torch.nn.AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
    "#        self.pool6 = torch.nn.AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
    "\n",
    "        # Fully Connected\n",
    "        self.fully_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64*13*13, 64),\n",
    "            torch.nn.BatchNorm1d(num_features=64),\n",
    "            torch.nn.ELU(inplace=True),\n",
    "            torch.nn.Linear(64, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.pool4(x)\n",
    "#        x = self.pool5(x)\n",
    "#        x = self.pool6(x)\n",
    "        \n",
    "        # flatten all dimensions except batch\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # classify with the final fully connected layers)\n",
    "        x = self.fully_net(x.reshape(x.shape[0], -1))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EQUIVARIANT = True\n",
    "dataname = 'xView'\n",
    "\n",
    "# 15 for DOTA, 62 for xView\n",
    "n_classes = -1\n",
    "if dataname == 'DOTAv1.0':\n",
    "    n_classes = 15\n",
    "elif dataname == 'xView':\n",
    "    n_classes = 62\n",
    "elif dataname == 'rotmnist':\n",
    "    n_classes = 10\n",
    "\n",
    "print(n_classes)\n",
    "assert n_classes > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EQUIVARIANT:\n",
    "    cyclic_group = 8\n",
    "    model = C8SteerableCNN(n_classes=n_classes, cyclic_group=cyclic_group).to(device)\n",
    "else:\n",
    "    cyclic_group = 1\n",
    "    model = NonRECNN(n_classes=n_classes).to(device)\n",
    "\n",
    "#summary(model, input_size=(3, 80, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cyclic_group = 8\n",
    "# n_classes = 15\n",
    "\n",
    "# model1 = C8SteerableCNN(n_classes=n_classes, cyclic_group=cyclic_group).to(device)\n",
    "# summary(model1, input_size=(3, 80, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model2 = NonRECNN(n_classes=n_classes).to(device)\n",
    "# summary(model2, input_size=(3, 80, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now randomly initialized. \n",
    "Therefore, we do not expect it to produce the right class probabilities.\n",
    "\n",
    "However, the model should still produce the same output for rotated versions of the same image.\n",
    "This is true for rotations by multiples of $\\frac{\\pi}{2}$, but is only approximate for rotations by $\\frac{\\pi}{4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the model on *rotated* MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download the dataset\n",
    "# !wget -nc http://www.iro.umontreal.ca/~lisa/icml2007data/mnist_rotation_new.zip\n",
    "# # uncompress the zip file\n",
    "# !unzip -n mnist_rotation_new.zip -d mnist_rotation_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# url = 'http://www.iro.umontreal.ca/~lisa/icml2007data/mnist_rotation_new.zip'\n",
    "# doc = requests.get(url)\n",
    "# with open('mnistrot.zip', 'wb') as f:\n",
    "#     f.write(doc.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode, transform=None, max_num_examples=999999, \n",
    "                 chip_size=80, dataname='xView', index_lists={}, classdict={}):\n",
    "        assert mode in ['train', 'val']\n",
    "        \n",
    "        #dataname: DOTAv1.0, xView, FAIR1M, etc\n",
    "        self.basedir = 'C:/Users/Admin/Desktop/data/'+dataname+'/'\n",
    "        self.chipdir = self.basedir+\"chips_\"+mode+\"/\"\n",
    "\n",
    "        self.transform = transform\n",
    "        self.max_num_examples = max_num_examples\n",
    "        #data = np.loadtxt(self.chipdir, delimiter=' ')\n",
    "        self.index_lists = index_lists\n",
    "        self.labels = []\n",
    "        self.chip_size = chip_size\n",
    "        self.color = (0,0,0)\n",
    "        self.classdict = classdict\n",
    "        self.classval = 0\n",
    "\n",
    "        for root, dirs, filenames in os.walk(self.chipdir, topdown=False):\n",
    "            pass\n",
    "\n",
    "        # first time through, we need to create self.index_lists and self.classdict\n",
    "        if not self.index_lists:\n",
    "            # first get all indices of each class and put them into dict self.index_lists\n",
    "            for d in dirs:\n",
    "                for root, dirs, filenames in os.walk(self.chipdir+d, topdown=False):\n",
    "                    pass\n",
    "                for f in filenames:\n",
    "                    if self.index_lists.get(d, False):\n",
    "                        self.index_lists[d].append(int(f[:-4]))\n",
    "                    else:\n",
    "                        self.index_lists[d] = [int(f[:-4])]\n",
    "\n",
    "        # now make self.classdict, or get it from when we made validation data\n",
    "        if not self.classdict:\n",
    "            # NOTE: labels must start at 0, not 1, or PyTorch is unhappy\n",
    "            for k in self.index_lists.keys():\n",
    "                self.classdict[k] = self.classval\n",
    "                self.classval += 1\n",
    "\n",
    "        # now take random sample from each list\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        #for k in self.index_lists.keys():\n",
    "        with tqdm(self.index_lists.keys(), unit=\"class\") as tkeys:\n",
    "            for k in tkeys:\n",
    "                ####################################################\n",
    "                # if too many examples, take self.max_num_examples random examples for that class\n",
    "                ####################################################\n",
    "                if len(self.index_lists[k]) > self.max_num_examples:\n",
    "                    temp = random.sample(self.index_lists[k], self.max_num_examples)\n",
    "\n",
    "                    for t in temp:\n",
    "                        self.images.append(self.prep_image(k, t))\n",
    "                        self.labels.append(classdict[k])\n",
    "                ####################################################\n",
    "                # if not enough examples, take all examples of that class\n",
    "                ####################################################\n",
    "                else:\n",
    "                    for t in self.index_lists[k]:\n",
    "                        self.images.append(self.prep_image(k, t))\n",
    "                        self.labels.append(classdict[k])\n",
    "        tkeys.close()\n",
    "\n",
    "\n",
    "    def prep_image(self, d, f):\n",
    "        imgstr = self.chipdir+d+\"/\"+str(f)+\".png\"\n",
    "        #print(imgstr)\n",
    "        #img = Image.open(imgstr)\n",
    "        #arr = np.asarray(img)\n",
    "        img = cv2.imread(imgstr)\n",
    "        old_image_height, old_image_width, channels = img.shape\n",
    "        #print('old', old_image_height, old_image_width)\n",
    "        \n",
    "        # first pad to become a square (then we'll resize later)\n",
    "        if img.shape[0] < img.shape[1]:\n",
    "            result1 = np.full((img.shape[1],img.shape[1], channels), self.color, dtype=np.uint8)\n",
    "            #print('shape', result1.shape)\n",
    "            # compute center offset\n",
    "            #x_center = np.abs(self.chip_size - old_image_width) // 2\n",
    "            y_center = np.abs(img.shape[1] - old_image_height) // 2\n",
    "            #print('y_center', y_center)\n",
    "            # copy img image into center of result image\n",
    "            result1[y_center:y_center+old_image_height, :] = img\n",
    "        elif img.shape[0] > img.shape[1]:\n",
    "            result1 = np.full((img.shape[0],img.shape[0], channels), self.color, dtype=np.uint8)\n",
    "            #print('shape', result1.shape)\n",
    "            # compute center offset\n",
    "            x_center = np.abs(img.shape[0] - old_image_width) // 2\n",
    "            #y_center = np.abs(self.chip_size - old_image_height) // 2\n",
    "            #print('x_center', x_center)\n",
    "            # copy img image into center of result image\n",
    "            result1[:, x_center:x_center+old_image_width] = img\n",
    "        else:\n",
    "            result1 = img[:,:,:]\n",
    "        \n",
    "        # now resize\n",
    "        if not (result1.shape[0] == self.chip_size and result1.shape[1] == self.chip_size):\n",
    "            #print('result1.shape1', result1.shape)\n",
    "            result1 = cv2.resize(result1, dsize=(self.chip_size, self.chip_size), interpolation=cv2.INTER_CUBIC)\n",
    "            #print('result1.shape2', result1.shape)\n",
    "        \n",
    "        res2 = result1.reshape(-1, self.chip_size, self.chip_size).astype(np.float32) # = arr[:, :-1].reshape(-1, self.chip_size, self.chip_size)\n",
    "        #print('res2.shape', res2.shape)\n",
    "\n",
    "        # ToTensor screws up the order, so we have to undo it:\n",
    "        # https://discuss.pytorch.org/t/torchvision-totensor-dont-change-channel-order/82038/2\n",
    "        #res2 = res2.permute((1, 2, 0)).contiguous()\n",
    "\n",
    "        # convert back to PIL Image object for pytorch transforms (e.g. RandomRotation) to work\n",
    "        #image = Image.fromarray(image)\n",
    "\n",
    "        #self.images.append(res2)\n",
    "        \n",
    "        return res2\n",
    "\n",
    "#         # labels\n",
    "#         # NOTE: labels must start at 0, not 1, or PyTorch is unhappy\n",
    "#         if self.classdict.get(d, -1) >= 0:\n",
    "#             self.labels.append(self.classdict[d])\n",
    "#         else: # it's not in self.classdict yet\n",
    "#             self.classdict[d] = self.classval\n",
    "#             self.classval += 1\n",
    "\n",
    "#         if self.max_num_examples > 0:\n",
    "#             #z1 = list(zip(self.images, self.labels))\n",
    "#             classdict = {}\n",
    "#             for i in range(len(self.labels)):\n",
    "#                 if classdict.get(self.labels[i], False):\n",
    "#                     classdict[self.labels[i]].append(self.images[i])\n",
    "#                 else:\n",
    "#                     classdict[self.labels[i]] = [self.images[i]]\n",
    "#             self.images = []\n",
    "#             self.labels = []\n",
    "#             for i in range(self.classval):\n",
    "#                 ####################################################\n",
    "#                 # if too many examples, take self.max_num_examples random examples for that class\n",
    "#                 ####################################################\n",
    "#                 if len(classdict[i]) > self.max_num_examples:\n",
    "#                     temp = random.sample(classdict[i], self.max_num_examples)\n",
    "\n",
    "#                     for t in temp:\n",
    "#                         self.images.append(t)\n",
    "#                         self.labels.append(i)\n",
    "#                 ####################################################\n",
    "#                 # if not enough examples, take all examples of that class\n",
    "#                 ####################################################\n",
    "#                 else:\n",
    "#                     for t in classdict[i]:\n",
    "#                         self.images.append(t)\n",
    "#                         self.labels.append(i)\n",
    "\n",
    "        self.num_samples = len(self.labels)\n",
    "        print(\"self.num_samples\", self.num_samples)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], self.labels[index]\n",
    "        # image is a numpy ndarray instead of PIL Image object\n",
    "        # NOTE: certain pytorch functions (aka RandomRotate) require PIL Image objects\n",
    "        # ToTensor screws up the shape/channel order, so we have to undo it:\n",
    "        # https://discuss.pytorch.org/t/torchvision-totensor-dont-change-channel-order/82038/2\n",
    "        #image = image.permute((1, 2, 0)).contiguous()\n",
    "        #image = Image.fromarray(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# images are padded to have shape 29x29.\n",
    "# this allows to use odd-size filters with stride 2 when downsampling a feature map in the model\n",
    "#pad = Pad((0, 0, 1, 1), fill=0)\n",
    "\n",
    "# to reduce interpolation artifacts (e.g. when testing the model on rotated images),\n",
    "# we upsample an image by a factor of 3, rotate it and finally downsample it again\n",
    "#resize1 = Resize(80*3)\n",
    "#resize2 = Resize(80)\n",
    "\n",
    "#totensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistRotDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mode, transform=None):\n",
    "        assert mode in ['train', 'test']\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            file = \"data/mnist_rotation_new/mnist_all_rotation_normalized_float_train_valid.amat\"\n",
    "        else:\n",
    "            file = \"data/mnist_rotation_new/mnist_all_rotation_normalized_float_test.amat\"\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        data = np.loadtxt(file, delimiter=' ')\n",
    "            \n",
    "        self.images = data[:, :-1].reshape(-1, 28, 28).astype(np.float32)\n",
    "        self.labels = data[:, -1].astype(np.int64)\n",
    "        self.num_samples = len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], self.labels[index]\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# images are padded to have shape 29x29.\n",
    "# this allows to use odd-size filters with stride 2 when downsampling a feature map in the model\n",
    "pad = Pad((0, 0, 1, 1), fill=0)\n",
    "\n",
    "# to reduce interpolation artifacts (e.g. when testing the model on rotated images),\n",
    "# we upsample an image by a factor of 3, rotate it and finally downsample it again\n",
    "resize1 = Resize(87)\n",
    "resize2 = Resize(29)\n",
    "\n",
    "totensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the test set\n",
    "vtest = TheDataset(mode='val', dataname=dataname, max_num_examples=1)\n",
    "#vtest = MnistRotDataset(mode='test')\n",
    "#print(vtest.index_lists['75'])\n",
    "print(vtest.classdict)\n",
    "del vtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the first image from the test set\n",
    "#x, y = next(iter(test))\n",
    "#print(x.shape)\n",
    "#print(test[3530])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: torch.nn.Module, x: Image):\n",
    "    # evaluate the `model` on 8 rotated versions of the input image `x`\n",
    "    model.eval()\n",
    "    totensor = ToTensor()\n",
    "    \n",
    "    wrmup = model(torch.randn(1, 3, 80, 80).to(device))\n",
    "    del wrmup\n",
    "    \n",
    "    #x = resize1(pad(x))\n",
    "    \n",
    "    print()\n",
    "    print('##########################################################################################')\n",
    "    header = 'angle |  ' + '  '.join([\"{:6d}\".format(d) for d in range(15)])\n",
    "    print(header)\n",
    "    with torch.no_grad():\n",
    "        for r in range(8):\n",
    "            #print(np.min(x), np.max(x), x.shape)\n",
    "            intimg = x.astype(np.uint8)\n",
    "            #print(type(intimg))\n",
    "            #print(np.min(intimg), np.max(intimg), intimg.shape)\n",
    "            # go from 3,80,80 -> 80,80,3\n",
    "            #intimg2 = intimg.permute((1, 2, 0)).contiguous() # can't permute numpy arrays\n",
    "            intimg2 = intimg.transpose(1, 2, 0)\n",
    "            #print(np.min(intimg2), np.max(intimg2), intimg2.shape)\n",
    "            img = Image.fromarray(intimg2)\n",
    "            #print(np.min(img), np.max(img))\n",
    "            rotimg = img.rotate(r*45., Image.BILINEAR)\n",
    "            #print(rotimg.shape)\n",
    "            x_transformed = totensor(rotimg).reshape(-1, 3, 80, 80)\n",
    "            x_transformed = x_transformed.to(device)\n",
    "\n",
    "            y = model(x_transformed)\n",
    "            y = y.to('cpu').numpy().squeeze()\n",
    "            \n",
    "            angle = r * 45\n",
    "            print(\"{:3d}:{}\".format(angle, y))\n",
    "    print('##########################################################################################')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "#test_model(model, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model is already almost invariant.\n",
    "However, we still observe small fluctuations in the outputs.\n",
    "\n",
    "This is because the model contains some operations which might break equivariance.\n",
    "For instance, every convolution includes a padding of $2$ pixels per side. This is adds information about the actual orientation of the grid where the image/feature map is sampled because the padding is not rotated with the image. \n",
    "\n",
    "During training, the model will observe rotated patterns and will learn to ignore the noise coming from the padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's train the model now.\n",
    "The model is exactly the same used to train a normal *PyTorch* architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataname: DOTAv1.0, xView, FAIR1M, etc\n",
    "#dataname = 'xView'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep the training dataset\n",
    "train_transform = Compose([\n",
    "    #pad,\n",
    "    #resize1,\n",
    "    #RandomRotation(180, resample=Image.BILINEAR, expand=False),\n",
    "    #resize2,\n",
    "    #ToTensor()\n",
    "])\n",
    "\n",
    "#data_train = MnistRotDataset(mode='train', transform=train_transform)\n",
    "data_train = TheDataset(mode='train', transform=train_transform, dataname=dataname)\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=8, shuffle=True)\n",
    "print(len(data_train), len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fresh_training_data(n, index_lists={}, classdict={}):\n",
    "    data_train = TheDataset(mode='train', transform=train_transform, max_num_examples=n, \n",
    "                            chip_size=80, dataname=dataname, index_lists=index_lists, classdict=classdict)\n",
    "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=8, shuffle=True, drop_last=True)\n",
    "    #print(len(data_train), len(train_loader))\n",
    "    return train_loader, data_train.index_lists, data_train.classdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prep the testing dataset\n",
    "val_transform = Compose([\n",
    "    #pad,\n",
    "    #ToTensor()\n",
    "])\n",
    "\n",
    "#data_val = MnistRotDataset(mode='test', transform=val_transform)\n",
    "data_val = TheDataset(mode='val', transform=val_transform, chip_size=80, dataname=dataname)\n",
    "val_loader = torch.utils.data.DataLoader(data_val, batch_size=8, shuffle=True, drop_last=True)\n",
    "print(len(data_val), \"labels\")\n",
    "print(len(val_loader), \"batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_is = []\n",
    "for v in val_loader:\n",
    "    label_is.append(v[1])\n",
    "    break\n",
    "print(label_is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini = 99999\n",
    "# maxi = 0\n",
    "# for m in data_train: # data_val\n",
    "#     if m[1] < mini:\n",
    "#         mini = m[1]\n",
    "#     if m[1] > maxi:\n",
    "#         maxi = m[1]\n",
    "# print(mini, maxi, \"<--- should be 0 14 instead of 1 15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_the_model(model, cyclic_group, epoch, train_acc, train_loss, test_acc, test_loss, dataname):\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.mkdir(\"models\")\n",
    "    model_name = \"models/model_\"+dataname+\"_C\"+str(cyclic_group)+\"_\"+str(epoch)+\"_\"+str(round(train_acc,4))+\"_\"+str(round(train_loss,4))+\"_\"+str(round(test_acc,4))+\"_\"+str(round(test_loss,4))+\".pth\"\n",
    "#    if not os.path.exists(\"models/model_\"+dataname+\".pth\"):\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "#    print('saved')\n",
    "#    else:\n",
    "#         model.load_state_dict(torch.load(\"models/model_\"+dataname+\".pth\"))\n",
    "#         print('loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_the_results(cyclic_group, epoch, train_accuracy, train_loss, test_accuracy, test_loss, dataname):\n",
    "    add_headers = False\n",
    "    if not os.path.exists(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    if not os.path.exists(\"results/model_\"+dataname+\".csv\"):\n",
    "        add_headers = True\n",
    "    # writing to csv file\n",
    "    with open(\"results/model_\"+dataname+\".csv\", 'a', newline='') as csvfile: \n",
    "        # creating a csv writer object \n",
    "        csvwriter = csv.writer(csvfile)\n",
    "\n",
    "        if add_headers:\n",
    "            csvwriter.writerow(['Dataname', 'Cyclic Group', 'Epoch', 'Train Accuracy', 'Train Loss', 'Test Accuracy', 'Test Loss'])\n",
    "\n",
    "        # writing the data rows\n",
    "        csvwriter.writerow([dataname, cyclic_group, epoch, train_accuracy, train_loss, test_accuracy, test_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# IF CONTINUING TRAINING, LOAD MODEL FROM LAST EPOCH\n",
    "######################################################\n",
    "CONTINUING = False\n",
    "if CONTINUING:\n",
    "    # train_loader = get_fresh_training_data(30000)\n",
    "    model.load_state_dict(torch.load(\"models/model_dota_C8_65_97.6337_874.7356_84.8987_2469.3884.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get initial performance with random weights\n",
    "# test_total = 0\n",
    "# test_correct = 0\n",
    "# test_loss = 0\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "    \n",
    "#     #for i, (x, t) in enumerate(val_loader):\n",
    "#     with tqdm(val_loader, unit=\"batch\") as tepoch:\n",
    "#         for x, t in tepoch:\n",
    "#             tepoch.set_description(f\"Epoch {-1}\")\n",
    "            \n",
    "#             #if i%1000==0:\n",
    "#             #    print(i, \"/\", len(test_loader))\n",
    "\n",
    "#             x = x.to(device)\n",
    "#             t = t.to(device)\n",
    "\n",
    "#             y = model(x)\n",
    "\n",
    "#             _, prediction = torch.max(y.data, 1)\n",
    "#             if prediction.shape[0] != t.shape[0]:\n",
    "#                 print(t)\n",
    "#                 t = t[:-(t.shape[0]-prediction.shape[0])]\n",
    "#                 print(t)\n",
    "#             test_total += t.shape[0]\n",
    "#             test_correct += (prediction == t).sum().item()\n",
    "\n",
    "#             loss = loss_function(y, t)\n",
    "#             test_loss += loss\n",
    "\n",
    "# test_accuracy = test_correct/test_total*100.\n",
    "    \n",
    "# print(f\"test accuracy: {test_accuracy}\")\n",
    "# print(f\"test loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples_per_class = 500\n",
    "max_per_class = 3000\n",
    "increase_by = 100\n",
    "\n",
    "index_lists = {}\n",
    "classdict = data_val.classdict.copy() # so we use the same labels for training later\n",
    "\n",
    "start_epoch = 0\n",
    "max_epochs = 100\n",
    "\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "    \n",
    "    print('starting epoch', epoch)\n",
    "    \n",
    "    ########################################\n",
    "    # TRAIN\n",
    "    ########################################\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # NOTE: i pull fresh training data examples each epoch to counter the huge class imbalance.\n",
    "    #       i keep the validation data the same each epoch though to make sure i'm consistently\n",
    "    #       measuring validation accuracy/loss.\n",
    "    train_loader, index_lists, classdict = get_fresh_training_data(samples_per_class,\n",
    "                                                                   index_lists=index_lists,\n",
    "                                                                   classdict=classdict)\n",
    "    print(\"samples_per_class\", samples_per_class)\n",
    "    # NOTE: increasing the number of samples per class will mess up the loss curve because i am\n",
    "    #       adding more examples and thus more potential loss each epoch. comment the next line out\n",
    "    #       if you wish to avoid this.\n",
    "    if samples_per_class < max_per_class:\n",
    "        samples_per_class += increase_by\n",
    "\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for x, t in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "\n",
    "            y = model(x)\n",
    "            _, prediction = torch.max(y.data, 1)\n",
    "\n",
    "            train_total += t.shape[0]\n",
    "            train_correct += (prediction == t).sum().item()\n",
    "\n",
    "            loss = loss_function(y, t)\n",
    "            train_loss += loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_accuracy = 100.*train_correct/train_total\n",
    "            tepoch.set_postfix(Train_Acc=train_accuracy, Train_Loss=train_loss.item())\n",
    "\n",
    "    tepoch.close()\n",
    "\n",
    "    ########################################\n",
    "    # TEST\n",
    "    ########################################\n",
    "    test_total = 0\n",
    "    test_correct = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        with tqdm(val_loader, unit=\"batch\") as tepoch:\n",
    "            for x, t in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "                x = x.to(device)\n",
    "                t = t.to(device)\n",
    "\n",
    "                y = model(x)\n",
    "                _, prediction = torch.max(y.data, 1)\n",
    "\n",
    "                test_total += t.shape[0]\n",
    "                test_correct += (prediction == t).sum().item()\n",
    "\n",
    "                loss = loss_function(y, t)\n",
    "                test_loss += loss\n",
    "\n",
    "                test_accuracy = 100.*test_correct/test_total\n",
    "                tepoch.set_postfix(Test_Acc=test_accuracy, Test_Loss=test_loss.item())\n",
    "\n",
    "    tepoch.close()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    save_the_results(cyclic_group, epoch, train_accuracy, train_loss.item(), test_accuracy, test_loss.item(), dataname)\n",
    "    save_the_model(model, cyclic_group, epoch, train_accuracy, train_loss.item(), test_accuracy, test_loss.item(), dataname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_the_results(cyclic_group, epoch, train_accuracy, train_loss.item(), test_accuracy, test_loss.item(), dataname)\n",
    "# save_the_model(model, cyclic_group, epoch, train_accuracy, train_loss.item(), test_accuracy, test_loss.item(), dataname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_accuracy = train_correct/train_total*100.\n",
    "# print(train_correct, train_total, train_accuracy)\n",
    "#save_the_results(cyclic_group, epoch, train_accuracy, train_loss.item(), test_accuracy, test_loss.item())\n",
    "#save_the_model(model, cyclic_group, epoch, train_accuracy, train_loss.item(), test_accuracy, test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prediction.shape, t.shape)\n",
    "# print(prediction)\n",
    "# print(t)\n",
    "# print(t[:-1])\n",
    "# print(y.shape)\n",
    "# print(torch.max(y.data, 1)) # 1 is the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch -1 | test accuracy: 10.011096469935502\n",
    "# epoch -1 | test loss: 2109988.5\n",
    "\n",
    "# epoch 0 | train accuracy: 52.91684784257414\n",
    "# epoch 0 | train loss: 42925.43359375\n",
    "# epoch 0 | test accuracy: 18.846660656078782\n",
    "# epoch 0 | test loss: 201328.765625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test = TheDataset(mode='test', dataname=dataname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the first image from the test set\n",
    "x, y = next(iter(data_val))\n",
    "\n",
    "# evaluate the model\n",
    "test_model(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_classes = 15 # 15 62\n",
    "\n",
    "confusion_matrix = torch.zeros(n_classes, n_classes)\n",
    "with torch.no_grad():\n",
    "#     for i, (x, t) in enumerate(val_loader):\n",
    "#         if i%1000==0:\n",
    "#             print(i, \"/\", len(val_loader))\n",
    "    with tqdm(val_loader, unit=\"batch\") as tepoch:\n",
    "        for x, t in tepoch:\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = model(x)\n",
    "            _, preds = torch.max(y, 1)\n",
    "            for t2, p in zip(t.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t2.long(), p.long()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "cm = confusion_matrix.numpy().copy()\n",
    "cm = cm.astype('int32')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, filenames in os.walk(data_val.chipdir, topdown=False):\n",
    "    pass\n",
    "print(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if EQUIVARIANT:\n",
    "    cnum = \"8\"\n",
    "else:\n",
    "    cnum = \"1\"\n",
    "\n",
    "dfcm = pd.DataFrame(cm,\n",
    "                    index = dirs,\n",
    "                    columns=dirs)\n",
    "plt.figure(figsize = (18,18))\n",
    "sns.heatmap(dfcm, annot=True, cmap=\"Blues\", fmt='d')\n",
    "plt.savefig(\"results/c\"+cnum+\"_confusion_matrix_\"+dataname+\"_counts.png\",\n",
    "            dpi=100,\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*confusion_matrix.diag()/confusion_matrix.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix.numpy().copy()\n",
    "#cm2 = cm2.astype('int32')\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(len(cm2)):\n",
    "    for col in range(len(cm2[row])):\n",
    "        cm2[row, col] = (100 * cm2[row, col]) / (confusion_matrix.sum(1)[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = cm2.astype('float32') #float32, int32\n",
    "#cm2 = cm2.astype('int32') #float32, int32\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EQUIVARIANT:\n",
    "    cnum = \"8\"\n",
    "else:\n",
    "    cnum = \"1\"\n",
    "\n",
    "dfcm2 = pd.DataFrame(cm2,\n",
    "                    index = dirs,\n",
    "                    columns=dirs)\n",
    "plt.figure(figsize = (18,18))\n",
    "sns.heatmap(dfcm2, annot=True, cmap=\"Blues\", fmt='.0f') # , fmt='.1f' 'd' 'g'\n",
    "plt.savefig(\"results/c\"+cnum+\"_confusion_matrix_\"+dataname+\"_accuracies.png\",\n",
    "            dpi=100,\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
