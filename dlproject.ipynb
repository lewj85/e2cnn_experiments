{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# General E(2)-Equivariant Steerable CNNs  -  A concrete example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build a **Steerable CNN** and try it MNIST.\n",
    "\n",
    "Let's also use a group a bit larger: we now build a model equivariant to $8$ rotations.\n",
    "We indicate the group of $N$ discrete rotations as $C_N$, i.e. the **cyclic group** of order $N$.\n",
    "In this case, we will use $C_8$.\n",
    "\n",
    "Because the inputs are still gray-scale images, the input type of the model is again a *scalar field*.\n",
    "\n",
    "However, internally we use *regular fields*: this is equivalent to a *group-equivariant convolutional neural network*.\n",
    "\n",
    "Finally, we build *invariant* features for the final classification task by pooling over the group using *Group Pooling*.\n",
    "\n",
    "The final classification is performed by a two fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "Here is the definition of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C8SteerableCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes=10):\n",
    "        \n",
    "        super(C8SteerableCNN, self).__init__()\n",
    "        \n",
    "        # the model is equivariant under rotations by 45 degrees, modelled by C8\n",
    "        self.r2_act = gspaces.Rot2dOnR2(N=8)\n",
    "        #print(\"self.r2_act\", self.r2_act)\n",
    "        \n",
    "        # the input image is a scalar field, corresponding to the trivial representation\n",
    "        in_type = nn.FieldType(self.r2_act, 3*[self.r2_act.trivial_repr])\n",
    "        #print(\"in_type\", in_type)\n",
    "        \n",
    "        # we store the input type for wrapping the images into a geometric tensor during the forward pass\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        # convolution 1\n",
    "        # first specify the output type of the convolutional layer\n",
    "        # we choose 24 feature fields, each transforming under the regular representation of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 24*[self.r2_act.regular_repr])\n",
    "        #print(\"out_type\", out_type)\n",
    "        self.block1 = nn.SequentialModule(\n",
    "            nn.MaskModule(in_type, 128, margin=1),\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=7, padding=1, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        #print('block1', self.block1.out_type.size)\n",
    "        \n",
    "        # convolution 2\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block1.out_type\n",
    "        # the output type of the second convolution layer are 48 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block2 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        #print('block2', self.block2.out_type.size)\n",
    "        self.pool1 = nn.SequentialModule(\n",
    "            nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        )\n",
    "        #print('pool1', self.pool1.out_type.size)\n",
    "        \n",
    "        # convolution 3\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block2.out_type\n",
    "        # the output type of the third convolution layer are 48 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block3 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        #print('block3', self.block3.out_type.size)\n",
    "        \n",
    "        # convolution 4\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block3.out_type\n",
    "        # the output type of the fourth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block4 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        #print('block4', self.block4.out_type.size)\n",
    "        self.pool2 = nn.SequentialModule(\n",
    "            nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        )\n",
    "        #print('pool2', self.pool2.out_type.size)\n",
    "        \n",
    "        # convolution 5\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block4.out_type\n",
    "        # the output type of the fifth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block5 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        #print('block5', self.block5.out_type.size)\n",
    "        \n",
    "        # convolution 6\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block5.out_type\n",
    "        # the output type of the sixth convolution layer are 64 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 64*[self.r2_act.regular_repr])\n",
    "        self.block6 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        #print('block6', self.block6.out_type.size)\n",
    "        self.pool3 = nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=1, padding=0)\n",
    "        #print('pool3', self.pool3.out_type.size)\n",
    "        \n",
    "        self.gpool = nn.GroupPooling(out_type) # pool3.out_type\n",
    "        \n",
    "        # number of output channels\n",
    "        c = self.gpool.out_type.size\n",
    "        #print('gpool', c)\n",
    "        \n",
    "        # Fully Connected\n",
    "        self.fully_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(c*25*25, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.ELU(inplace=True),\n",
    "            torch.nn.Linear(64, n_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # wrap the input tensor in a GeometricTensor\n",
    "        # (associate it with the input type)\n",
    "        #print(\"input.shape\", input.shape)\n",
    "        x = nn.GeometricTensor(input, self.input_type)\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        \n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        \n",
    "        # pool over the spatial dimensions\n",
    "        x = self.pool3(x)\n",
    "        #print(\"x.shape1\", x.shape)\n",
    "        \n",
    "        # pool over the group\n",
    "        x = self.gpool(x)\n",
    "        #print(\"x.shape2\", x.shape)\n",
    "\n",
    "        # unwrap the output GeometricTensor\n",
    "        # (take the Pytorch tensor and discard the associated representation)\n",
    "        x = x.tensor\n",
    "        #print(\"x.shape3\", x.shape)\n",
    "        \n",
    "        # classify with the final fully connected layers)\n",
    "        x = self.fully_net(x.reshape(x.shape[0], -1)) # x.shape[1]?\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the model on *rotated* MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download the dataset\n",
    "# !wget -nc http://www.iro.umontreal.ca/~lisa/icml2007data/mnist_rotation_new.zip\n",
    "# # uncompress the zip file\n",
    "# !unzip -n mnist_rotation_new.zip -d mnist_rotation_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# url = 'http://www.iro.umontreal.ca/~lisa/icml2007data/mnist_rotation_new.zip'\n",
    "# doc = requests.get(url)\n",
    "# with open('mnistrot.zip', 'wb') as f:\n",
    "#     f.write(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import RandomRotation\n",
    "from torchvision.transforms import Pad\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DOTARotDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mode, transform=None):\n",
    "        assert mode in ['train', 'test']\n",
    "        \n",
    "        basedir = 'C:/Users/Admin/Desktop/data/DOTAv1.0/'\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            file = basedir+\"chips_train/\"\n",
    "        else:\n",
    "            file = basedir+\"chips_val/\"\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        new_image_width = 128\n",
    "        new_image_height = 128\n",
    "        color = (0,0,0)\n",
    "        self.classdict = {}\n",
    "        self.classval = 0\n",
    "\n",
    "        #data = np.loadtxt(file, delimiter=' ')\n",
    "        data = []\n",
    "        \n",
    "        for root, dirs, filenames in os.walk(file, topdown=False):\n",
    "            pass\n",
    "        for d in dirs:\n",
    "            for root, dirs, filenames in os.walk(file+d, topdown=False):\n",
    "                pass\n",
    "            print(d, len(filenames))\n",
    "            for f in filenames:\n",
    "                imgstr = file+d+\"/\"+f\n",
    "                #img = Image.open(imgstr)\n",
    "                #arr = np.asarray(img)\n",
    "                img = cv2.imread(imgstr)\n",
    "                old_image_height, old_image_width, channels = img.shape\n",
    "                #print('old', old_image_height, old_image_width)\n",
    "                if img.shape[0] < img.shape[1]:\n",
    "                    result1 = np.full((img.shape[1],img.shape[1], channels), color, dtype=np.uint8)\n",
    "                    #print('shape', result1.shape)\n",
    "                    # compute center offset\n",
    "                    #x_center = np.abs(new_image_width - old_image_width) // 2\n",
    "                    y_center = np.abs(img.shape[1] - old_image_height) // 2\n",
    "                    #print('y_center', y_center)\n",
    "                    # copy img image into center of result image\n",
    "                    result1[y_center:y_center+old_image_height, :] = img\n",
    "                elif img.shape[0] > img.shape[1]:\n",
    "                    result1 = np.full((img.shape[0],img.shape[0], channels), color, dtype=np.uint8)\n",
    "                    #print('shape', result1.shape)\n",
    "                    # compute center offset\n",
    "                    x_center = np.abs(img.shape[0] - old_image_width) // 2\n",
    "                    #y_center = np.abs(new_image_height - old_image_height) // 2\n",
    "                    #print('x_center', x_center)\n",
    "                    # copy img image into center of result image\n",
    "                    result1[:, x_center:x_center+old_image_width] = img\n",
    "                else:\n",
    "                    result1 = img[:,:,:]\n",
    "                # check if we need to resize\n",
    "                if not (result1.shape[0] == 128 and result1.shape[1] == 128):\n",
    "                    #print('result1.shape1', result1.shape)\n",
    "                    result1 = cv2.resize(result1, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "                    #print('result1.shape2', result1.shape)\n",
    "                res2 = result1.reshape(-1, 128, 128).astype(np.float32) # = arr[:, :-1].reshape(-1, 128, 128)\n",
    "                #print('res2.shape', res2.shape)\n",
    "                \n",
    "                # ToTensor screws up the order, so we have to undo it:\n",
    "                # https://discuss.pytorch.org/t/torchvision-totensor-dont-change-channel-order/82038/2\n",
    "                #res2 = res2.permute((1, 2, 0)).contiguous()\n",
    "                \n",
    "                self.images.append(res2)\n",
    "        \n",
    "                # labels\n",
    "                if self.classdict.get(d, False):\n",
    "                    self.labels.append(self.classdict[d])\n",
    "                else: # it's not in self.classdict yet\n",
    "                    self.classdict[d] = self.classval\n",
    "                    self.classval += 1\n",
    "        self.num_samples = len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], self.labels[index]\n",
    "        #image = Image.fromarray(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# images are padded to have shape 29x29.\n",
    "# this allows to use odd-size filters with stride 2 when downsampling a feature map in the model\n",
    "pad = Pad((0, 0, 1, 1), fill=0)\n",
    "\n",
    "# to reduce interpolation artifacts (e.g. when testing the model on rotated images),\n",
    "# we upsample an image by a factor of 3, rotate it and finally downsample it again\n",
    "resize1 = Resize(128*3)\n",
    "resize2 = Resize(128)\n",
    "\n",
    "totensor = ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Desktop\\school\\e2cnn_experiments\\e2cnn\\nn\\modules\\r2_conv\\basisexpansion_singleblock.py:80: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  full_mask[mask] = norms.to(torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "model = C8SteerableCNN(n_classes=15).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "        MaskModule-1          [-1, 3, 128, 128]               0\n",
      "SingleBlockBasisExpansion-2             [-1, 8, 1, 49]               0\n",
      "BlocksBasisExpansion-3                [-1, 3, 49]               0\n",
      "            R2Conv-4        [-1, 192, 124, 124]               0\n",
      "       BatchNorm3d-5      [-1, 24, 8, 124, 124]              48\n",
      "    InnerBatchNorm-6        [-1, 192, 124, 124]               0\n",
      "              ReLU-7        [-1, 192, 124, 124]               0\n",
      "  SequentialModule-8        [-1, 192, 124, 124]               0\n",
      "SingleBlockBasisExpansion-9             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-10             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-11             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-12             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-13             [-1, 8, 8, 25]               0\n",
      "BlocksBasisExpansion-14              [-1, 192, 25]               0\n",
      "           R2Conv-15        [-1, 384, 124, 124]               0\n",
      "      BatchNorm3d-16      [-1, 48, 8, 124, 124]              96\n",
      "   InnerBatchNorm-17        [-1, 384, 124, 124]               0\n",
      "             ReLU-18        [-1, 384, 124, 124]               0\n",
      " SequentialModule-19        [-1, 384, 124, 124]               0\n",
      "PointwiseAvgPoolAntialiased-20          [-1, 384, 62, 62]               0\n",
      " SequentialModule-21          [-1, 384, 62, 62]               0\n",
      "SingleBlockBasisExpansion-22             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-23             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-24             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-25             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-26             [-1, 8, 8, 25]               0\n",
      "BlocksBasisExpansion-27              [-1, 384, 25]               0\n",
      "           R2Conv-28          [-1, 384, 62, 62]               0\n",
      "      BatchNorm3d-29        [-1, 48, 8, 62, 62]              96\n",
      "   InnerBatchNorm-30          [-1, 384, 62, 62]               0\n",
      "             ReLU-31          [-1, 384, 62, 62]               0\n",
      " SequentialModule-32          [-1, 384, 62, 62]               0\n",
      "SingleBlockBasisExpansion-33             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-34             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-35             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-36             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-37             [-1, 8, 8, 25]               0\n",
      "BlocksBasisExpansion-38              [-1, 384, 25]               0\n",
      "           R2Conv-39          [-1, 768, 62, 62]               0\n",
      "      BatchNorm3d-40        [-1, 96, 8, 62, 62]             192\n",
      "   InnerBatchNorm-41          [-1, 768, 62, 62]               0\n",
      "             ReLU-42          [-1, 768, 62, 62]               0\n",
      " SequentialModule-43          [-1, 768, 62, 62]               0\n",
      "PointwiseAvgPoolAntialiased-44          [-1, 768, 31, 31]               0\n",
      " SequentialModule-45          [-1, 768, 31, 31]               0\n",
      "SingleBlockBasisExpansion-46             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-47             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-48             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-49             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-50             [-1, 8, 8, 25]               0\n",
      "BlocksBasisExpansion-51              [-1, 768, 25]               0\n",
      "           R2Conv-52          [-1, 768, 31, 31]               0\n",
      "      BatchNorm3d-53        [-1, 96, 8, 31, 31]             192\n",
      "   InnerBatchNorm-54          [-1, 768, 31, 31]               0\n",
      "             ReLU-55          [-1, 768, 31, 31]               0\n",
      " SequentialModule-56          [-1, 768, 31, 31]               0\n",
      "SingleBlockBasisExpansion-57             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-58             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-59             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-60             [-1, 8, 8, 25]               0\n",
      "SingleBlockBasisExpansion-61             [-1, 8, 8, 25]               0\n",
      "BlocksBasisExpansion-62              [-1, 768, 25]               0\n",
      "           R2Conv-63          [-1, 512, 29, 29]               0\n",
      "      BatchNorm3d-64        [-1, 64, 8, 29, 29]             128\n",
      "   InnerBatchNorm-65          [-1, 512, 29, 29]               0\n",
      "             ReLU-66          [-1, 512, 29, 29]               0\n",
      " SequentialModule-67          [-1, 512, 29, 29]               0\n",
      "PointwiseAvgPoolAntialiased-68          [-1, 512, 25, 25]               0\n",
      "     GroupPooling-69           [-1, 64, 25, 25]               0\n",
      "           Linear-70                   [-1, 64]       2,560,064\n",
      "      BatchNorm1d-71                   [-1, 64]             128\n",
      "              ELU-72                   [-1, 64]               0\n",
      "           Linear-73                   [-1, 15]             975\n",
      "================================================================\n",
      "Total params: 2,561,919\n",
      "Trainable params: 2,561,919\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 589.05\n",
      "Params size (MB): 9.77\n",
      "Estimated Total Size (MB): 599.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now randomly initialized. \n",
    "Therefore, we do not expect it to produce the right class probabilities.\n",
    "\n",
    "However, the model should still produce the same output for rotated versions of the same image.\n",
    "This is true for rotations by multiples of $\\frac{\\pi}{2}$, but is only approximate for rotations by $\\frac{\\pi}{4}$.\n",
    "\n",
    "Let's test it on a random test image:\n",
    "we feed eight rotated versions of the first image in the test set and print the output logits of the model for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseball-diamond 73\n",
      "basketball-court 111\n",
      "bridge 2\n",
      "ground-track-field 19\n",
      "harbor 2307\n",
      "helicopter 18\n",
      "large-vehicle 3164\n",
      "plane 1543\n",
      "roundabout 50\n",
      "ship 15846\n",
      "small-vehicle 7673\n",
      "soccer-ball-field 52\n",
      "storage-tank 58\n",
      "swimming-pool 1019\n",
      "tennis-court 1133\n"
     ]
    }
   ],
   "source": [
    "# build the test set\n",
    "raw_mnist_test = DOTARotDataset(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 128, 128)\n",
      "(array([[[  0.,   0.,   0., ...,  57.,  12.,  13.],\n",
      "        [ 11.,   3.,   4., ...,  38.,  29.,  17.],\n",
      "        [ 12.,   4.,   9., ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [  5.,   4.,   7., ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., ...,  12.,  46.,  35.],\n",
      "        [ 28.,  29.,  15., ...,  50.,  49.,   9.]],\n",
      "\n",
      "       [[  8.,   7.,  38., ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., ...,   6.,  37.,  23.],\n",
      "        [ 17.,  24.,   7., ...,  14.,  14.,  22.],\n",
      "        ...,\n",
      "        [ 58.,  63.,  58., ..., 141., 136., 149.],\n",
      "        [148., 144., 157., ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., ...,  49.,  50.,  44.]],\n",
      "\n",
      "       [[ 47.,  63.,  57., ..., 143., 138., 153.],\n",
      "        [151., 147., 156., ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., ...,  29.,  40.,  32.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0., ...,  39.,  29.,  20.],\n",
      "        [ 11.,  50.,  45., ..., 134., 129., 130.],\n",
      "        [136., 131., 141., ...,   0.,   0.,   0.]]], dtype=float32), 7)\n"
     ]
    }
   ],
   "source": [
    "# retrieve the first image from the test set\n",
    "x, y = next(iter(raw_mnist_test))\n",
    "\n",
    "print(x.shape)\n",
    "print(raw_mnist_test[3530])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: torch.nn.Module, x: Image):\n",
    "    # evaluate the `model` on 8 rotated versions of the input image `x`\n",
    "    model.eval()\n",
    "    \n",
    "    wrmup = model(torch.randn(1, 3, 128, 128).to(device))\n",
    "    del wrmup\n",
    "    \n",
    "    #x = resize1(pad(x))\n",
    "    \n",
    "    print()\n",
    "    print('##########################################################################################')\n",
    "    header = 'angle |  ' + '  '.join([\"{:6d}\".format(d) for d in range(10)])\n",
    "    print(header)\n",
    "    with torch.no_grad():\n",
    "        for r in range(8):\n",
    "            #print(np.min(x), np.max(x), x.shape)\n",
    "            intimg = x.astype(np.uint8)\n",
    "            #print(type(intimg))\n",
    "            print(np.min(intimg), np.max(intimg), intimg.shape)\n",
    "            img = Image.fromarray(intimg)\n",
    "            #print(np.min(img), np.max(img))\n",
    "            rotimg = img.rotate(r*45., Image.BILINEAR)\n",
    "            #print(rotimg.shape)\n",
    "            x_transformed = totensor(rotimg).reshape(-1, 3, 128, 128)\n",
    "            x_transformed = x_transformed.to(device)\n",
    "\n",
    "            y = model(x_transformed)\n",
    "            y = y.to('cpu').numpy().squeeze()\n",
    "            \n",
    "            angle = r * 45\n",
    "            print(\"{:5d} : {}\".format(angle, y))\n",
    "    print('##########################################################################################')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########################################################################################\n",
      "angle |       0       1       2       3       4       5       6       7       8       9\n",
      "0 249 (3, 128, 128)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 128), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch38\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2834\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2835\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2836\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 128), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2480/66125697.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2480/3284462941.py\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(model, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m#print(type(intimg))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;31m#print(np.min(img), np.max(img))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mrotimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m45.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBILINEAR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch38\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2835\u001b[0m             \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2836\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2837\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot handle this data type: %s, %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtypekey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2838\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2839\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 128), |u1"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "test_model(model, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model is already almost invariant.\n",
    "However, we still observe small fluctuations in the outputs.\n",
    "\n",
    "This is because the model contains some operations which might break equivariance.\n",
    "For instance, every convolution includes a padding of $2$ pixels per side. This is adds information about the actual orientation of the grid where the image/feature map is sampled because the padding is not rotated with the image. \n",
    "\n",
    "During training, the model will observe rotated patterns and will learn to ignore the noise coming from the padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's train the model now.\n",
    "The model is exactly the same used to train a normal *PyTorch* architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseball-diamond 58\n",
      "basketball-court 50\n",
      "bridge 2\n",
      "ground-track-field 18\n",
      "harbor 703\n",
      "helicopter 18\n",
      "large-vehicle 2624\n",
      "plane 1428\n",
      "roundabout 44\n",
      "ship 652\n",
      "small-vehicle 6189\n",
      "soccer-ball-field 48\n",
      "storage-tank 55\n",
      "swimming-pool 967\n",
      "tennis-court 682\n"
     ]
    }
   ],
   "source": [
    "# Prep the training dataset\n",
    "train_transform = Compose([\n",
    "    #pad,\n",
    "    #resize1,\n",
    "    #RandomRotation(180, resample=Image.BILINEAR, expand=False),\n",
    "    #resize2,\n",
    "    #totensor\n",
    "])\n",
    "\n",
    "mnist_train = DOTARotDataset(mode='train', transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseball-diamond 74\n",
      "basketball-court 99\n",
      "bridge 297\n",
      "ground-track-field 38\n",
      "harbor 1977\n",
      "helicopter 66\n",
      "large-vehicle 1254\n",
      "plane 1722\n",
      "roundabout 45\n",
      "ship 8141\n",
      "small-vehicle 3270\n",
      "soccer-ball-field 43\n",
      "storage-tank 443\n",
      "swimming-pool 433\n",
      "tennis-court 382\n"
     ]
    }
   ],
   "source": [
    "# Prep the testing dataset\n",
    "test_transform = Compose([\n",
    "    #pad,\n",
    "    #totensor,\n",
    "])\n",
    "mnist_test = DOTARotDataset(mode='test', transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | test accuracy: 6.86993650098533\n"
     ]
    }
   ],
   "source": [
    "# get initial performance with random weights\n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, (x, t) in enumerate(test_loader):\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = model(x)\n",
    "        \n",
    "        _, prediction = torch.max(y.data, 1)\n",
    "        total += t.shape[0]\n",
    "        correct += (prediction == t).sum().item()\n",
    "print(f\"epoch {epoch} | test accuracy: {correct/total*100.}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for i, (x, t) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = model(x)\n",
    "\n",
    "        loss = loss_function(y, t)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    #if epoch % 10 == 0:\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (x, t) in enumerate(test_loader):\n",
    "\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "\n",
    "            y = model(x)\n",
    "\n",
    "            _, prediction = torch.max(y.data, 1)\n",
    "            total += t.shape[0]\n",
    "            correct += (prediction == t).sum().item()\n",
    "    print(f\"epoch {epoch} | train accuracy: {correct/total*100.}\")\n",
    "    print(f\"epoch {epoch} | train loss: {correct/total*100.}\")\n",
    "    print(f\"epoch {epoch} | test accuracy: {correct/total*100.}\")\n",
    "    print(f\"epoch {epoch} | test loss: {correct/total*100.}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########################################################################################\n",
      "angle |       0       1       2       3       4       5       6       7       8       9\n",
      "    0 : [-0.2373 -0.5674 -1.0786 -0.5599  0.1627  0.3779  4.1019  0.4574 -1.1484 -0.3254]\n",
      "   45 : [-0.5487 -0.4267 -1.1523 -0.7452  0.1651  0.327   4.0815  0.5293 -0.8953 -0.3933]\n",
      "   90 : [-0.2373 -0.5674 -1.0786 -0.5599  0.1627  0.3779  4.1019  0.4574 -1.1484 -0.3254]\n",
      "  135 : [-0.5487 -0.4267 -1.1523 -0.7452  0.1651  0.327   4.0815  0.5293 -0.8953 -0.3933]\n",
      "  180 : [-0.2373 -0.5674 -1.0786 -0.5599  0.1627  0.3779  4.1019  0.4574 -1.1484 -0.3254]\n",
      "  225 : [-0.5487 -0.4267 -1.1523 -0.7452  0.1651  0.3271  4.0815  0.5293 -0.8953 -0.3933]\n",
      "  270 : [-0.2373 -0.5674 -1.0786 -0.5599  0.1627  0.3779  4.1019  0.4574 -1.1484 -0.3254]\n",
      "  315 : [-0.5487 -0.4267 -1.1523 -0.7452  0.1651  0.327   4.0815  0.5293 -0.8953 -0.3933]\n",
      "##########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrieve the first image from the test set\n",
    "x, y = next(iter(raw_mnist_test))\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "test_model(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
